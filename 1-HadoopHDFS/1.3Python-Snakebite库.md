<h2>Python-Snakebite库</h2>

Snakebite是由Spotify创建的python包, 它提供了python客户端库,运行以编程方式从Python应用程序访问HDFS。客户端库使用 protobuf 消息直接与 NameNode 通信。Snakebite还包括一个基于客户端库的 HDFS 的命令行界面。

本节介绍如何安装和配置SnakeBite包。对Snakebite的客户端库进行了详细的解释, 其中有多个示例, Snakebite的内置CLI被作为python非必要hdfs命令工具而介绍。


<h3>安装</h3>

Snakebite目前只支持Python2,需要Python-protobuf最低版本为2.4.1;python3目前还不支持。Snakebite通过PyPI进行分发，可以使用pip进行安装:

```
$ pip install snakebite
```

<h3>客户端库</h3>

客户端库是用 Python 编写的, 使用 protobuf 消息, 并实现 Hadoop RPC 协议来与 NameNode 通信。这使得 Python 应用程序可以直接与 HDFS 通信, 而不必对 HDFS dfs 进行系统调用。

<h4>列举目录信息</h4>

* Example 1-1.python/HDFS/list_directory.py
使用Snakebite客户端列举在HDFS根目录下的信息:

```
from snakebite.client import Client
client = Client('localhost', 9000)
for x in client.ls(['/']):
    print x
```

以上代码中最重要的行，也是其他使用Snakebite客户端的程序中都需要的是 创建客户端到HDFS的NameNode的连接:

```
client = Client('localhost', 9000)
```

对于client方法接受以下的参数

* host(string) : NameNode的主机名或Ip地址。
* prot(int)    : NameNode RPC端口号
* hadoop_version(int): 使用的Hadoop协议的版本号
* use_trash(boolean): 删除文件时，使用回收站
* effective_use(string): 对用户操作的有效用户

主机和端口参数是必需的, 它们的值为依赖于 HDFS 配置。这些值参数可以在fs.default属性中找到的 ,在文件hadoop/conf/core-site.xml中。

```
<property>
<name>fs.defaultFS</name>
<value>hdfs://localhost:9000</value>
</property>
```

对于本节中的示例, 主机和端口所使用的值分别为本地主机和9000。创建客户端连接后, 可以访问HDFS文件系统。上一个应用程序的使用ls命令列出HDFS中根目录的内容。

```
for x in client.ls(['/']):
    print x
```

重要的是要注意到, Snakebite中的许多方法可迭代对象。因此, 必须迭代它们才能执行。ls方法获取路径列表, 并返回包含文件信息的映射列表。

执行list_directory.py脚本后的返回的结果为:

```
$ python list_directory.py
{'group': u'supergroup', 'permission': 448, 'file_type': 'd',
'access_time': 0L, 'block_replication': 0, 'modification_time': 1442752574936L, 'length': 0L, 'blocksize': 0L,
'owner': u'hduser', 'path': '/tmp'}
{'group': u'supergroup', 'permission': 493, 'file_type': 'd',
'access_time': 0L, 'block_replication': 0, 'modification_time': 1442742056276L, 'length': 0L, 'blocksize': 0L,
'owner': u'hduser', 'path': '/user'}
```

<h4>创建目录</h4>

使用mkdir()方法在HDFS上创建目录 ， Example1-2 在HDFS上创建目录/foo/bar和/input

* Example 1-2.python/HDFS/mkdir.py

```
from snakebite.client import Client
client = Client('localhost', 9000)
for p in client.mkdir(['/foo/bar', '/input'], create_parent=True):
    print p
```

执行mkdir.py脚本，生成以下的结果:

```
$ python mkdir.py
{'path': '/foo/bar', 'result': True}
{'path': '/input', 'result': True}
```

mkdir()方法接受路径队列，在HDFS上创建对应的路径,上述示例中使用 **create_parent** 参数确保在创建目录时其父目录是否存在。设置 **create_parent** 为True类型Unix中的 **mkdir -p** 命令。

<h4>删除文件和目录</h4>

通过delete()方法可以从HDFS上删除文件和目录。Example 1-3 从文件夹/foo , /bar 中地柜的删除文件。

* Example 1-3. python/HDFS/delete.py

```
from snakebite.client import Client
client = Client('localhost', 9000)
for p in client.delete(['/foo', '/input'], recurse=True):
    print p
```

执行delete.py脚本后，生成的结果:

```
$ python delete.py
{'path': '/foo', 'result': True}
{'path': '/input', 'result': True}
```

执行递归删除将删除目录包含的任何子目录和文件。如果找不到指定的路径, 则 delete 方法将引发则抛出。如果未指定递归, 并且存在子目录或文件将引发DirectoryException异常。

与递归等价的命令参数是 **rm -rf**,对于这个命令应该谨慎使用。

<h4>从HDFS上获取数据</h4>

与 hdfs dfs 命令一样, 客户端库包含多个允许从 HDFS 中检索数据的方法。复制文件从 HDFS 到本地文件系统, 使用 copyToLocal()方法。示例1-4从HDFS复制文件/input/input.txt, 并将其置于本地文件系统的/tmp目录下。

* Example 1-4.python/HDFS/copy_to_local.py

```
from snakebite.client import Client
client = Client('localhost', 9000)
for f in client.copyToLocal(['/input/input.txt'], '/tmp'):
    print f
```
执行脚本 copy_to_local.py,返回的结果数据:

```
$ python copy_to_local.py
{'path': '/tmp/input.txt', 'source_path': '/input/input.txt',
'result': True, 'error': ''}
```

只是简单的读取HDFS上文本的内容，可以使用text()方法,Example1-5显示/input/input.txt文本内容。

* Example 1-5. python/HDFS/text.py

```
from snakebite.client import Client
client = Client('localhost', 9000)
for l in client.text(['/input/input.txt']):
    print l
```

执行text.py脚本后，生成以下结果:

```
$ python text.py
jack be nimble
jack be quick
jack jumped over the candlestick
```

text()方法会自动解压Gzip和bzip2文件。

<h4>CLI Client</h4>

Snakebite的CLI客户端是一个Python命令行HDFS客户端（基于客户端库）。在运行Snakebite客户端时，主机名或IP，端口号都必须指定。有很多方法指定这些值，最简单的方法是方法是创建一个 ~.snakebite配置文件。Example1-6包含一个配置示例。

* Example 1-6. ~/.snakebiterc

```
{
"config_version": 2,
"skiptrash": true,
"namenodes": 
 [
    {"host": "localhost", "port": 9000, "version": 9},
 ]
}
```

这些配置信息可以在配置文件hadoop/conf/coresite.xml中的fs.defaultFS属性中找到。

<h4>使用CLI Client</h4>

通过命令行使用CLI Client,使用ls命令显示目录信息:

```
$ snakebite ls /
Found 2 items
drwx------ - hadoop supergroup 0 2015-09-20 14:36 /tmp
drwxr-xr-x - hadoop supergroup 0 2015-09-20 11:40 /user
```

和HDFS dfs命令一样，CLI Client同样支持很多相似文件操作命令(ls,mkdir,df,du等)

Snakebite和hdfs dfs主要不同是，snakebite是纯python客户端，和HDFS通信是不需要加载任何的Java库,这样将能更快的解析命令。

